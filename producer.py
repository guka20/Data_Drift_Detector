import pandas as pd
from confluent_kafka import Producer
import json
from datetime import datetime

# Kafka-áƒ¡ áƒ™áƒáƒœáƒ¤áƒ˜áƒ’áƒ£áƒ áƒáƒªáƒ˜áƒ
conf = {'bootstrap.servers': '127.0.0.1:9092'}
producer = Producer(conf)

# áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ”áƒ‘áƒ˜áƒ¡ áƒ¬áƒáƒ›áƒáƒ¦áƒ”áƒ‘áƒ IOT-temp.csv-áƒ“áƒáƒœ
print("Loading IoT temperature dataset...")
df = pd.read_csv('./dataset/IOT-temp.csv')

print(f"Loaded {len(df)} records")
print(f"Date range: {df['noted_date'].min()} to {df['noted_date'].max()}")

events_sent = 0
failed = 0

print("Transforming and sending to Kafka...")

for idx, row in df.iterrows():
    try:
        
        timestamp = datetime.strptime(row['noted_date'], '%d-%m-%Y %H:%M')

        #áƒ¨áƒ”áƒ¥áƒ•áƒ›áƒœáƒáƒ“ áƒáƒ˜áƒ“áƒ˜ indoor áƒáƒœ outdoor 
        location = row['out/in'].strip().lower()
        source_id = f"room_admin_{location}door" 
        
        # event-áƒ˜áƒ¡ áƒ¨áƒ”áƒ¥áƒ›áƒœáƒ
        event = {
            "source_id": source_id,
            "metric": "temperature",
            "value": float(row['temp']),
            "event_time": timestamp.strftime("%Y-%m-%dT%H:%M:%S.000Z")
        }
        
        # kafka-áƒ¨áƒ˜ áƒ’áƒáƒ“áƒáƒ’áƒ–áƒáƒ•áƒœáƒ
        producer.produce('telemetry.events', json.dumps(event).encode('utf-8'))
        events_sent += 1
        
        # áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ áƒáƒ”áƒ áƒ˜áƒáƒ“áƒ£áƒšáƒáƒ“
        # áƒ—áƒ˜áƒ—áƒ áƒ”áƒ•áƒ”áƒœáƒ—áƒ˜áƒ¡ áƒ’áƒáƒ¡áƒáƒ’áƒ–áƒáƒ•áƒœáƒáƒ“ áƒ“áƒáƒ¡áƒ­áƒ˜áƒ áƒ“áƒ”áƒ‘áƒ áƒ“áƒ˜áƒ“áƒ˜ áƒ“áƒ áƒ
        # áƒáƒ›áƒ˜áƒ¢áƒáƒ› áƒ•áƒáƒ¯áƒ’áƒ£áƒ¤áƒ”áƒ‘áƒ— áƒ“áƒ áƒ•áƒáƒ’áƒ–áƒáƒ•áƒœáƒ˜áƒ— áƒ§áƒáƒ•áƒ”áƒšáƒ— 5000 event-áƒ¡ áƒ”áƒ áƒ—áƒáƒ“
        if events_sent % 5000 == 0:
            producer.flush()
            progress = (events_sent / len(df)) * 100
            print(f"ğŸ“¤ Sent {events_sent:,} / {len(df):,} events ({progress:.1f}%)")
        
    except Exception as e:
        failed += 1
        if failed < 10: 
            print(f"âš ï¸  Error processing row {idx}: {e}")

producer.flush()